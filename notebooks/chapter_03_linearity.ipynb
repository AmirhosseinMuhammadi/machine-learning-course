{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ae70fb-1c17-4edb-94a7-70088dec451f",
   "metadata": {},
   "source": [
    "# Chapter 3: Linearity\n",
    "Linearity is a foundational concept in machine learning, characterizing models where the output is a direct, weighted sum of the input features. While simple and highly interpretable, linear models like Linear and Logistic Regression make strong assumptions about the underlying data structure. Their performance is often limited when faced with complex, non-linear relationships. However, the principle of linearity remains profoundly influential, serving as the essential building block within sophisticated non-linear architectures such as neural networks and kernel methods. This chapter covers fundamental concepts through building end-to-end projects based on the machine learning life cycle.\n",
    "\n",
    "## Table of Contents\n",
    "- [Linear Regression](#linear-regression)\n",
    "- [Gradient Descent](#gradient-descent)\n",
    "- [Stochastic Gradient Descent](#stochastic-gradient-descent)\n",
    "- [Linear Classification (Logistic Regression)](#linear-classification-logistic-regression)\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed5d15-092c-4e45-8d9b-3afe8f8cd4f5",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Linear regression is the primary supervised learning algorithm that fits a linear function to the data. It is a statistical method that can be used to solve multi-dimensional problems. The number of features (independent variables) determines the **feature space dimensionality (input space)**.\n",
    "\n",
    "### Simple Linear Regression\n",
    "In a **1D feature space**, the simple linear regression is mathematically expressed as the best straight line.\n",
    "$$ \\hat{y} = w_1x + w_0 $$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{y} $ is the **fitted** line (predicted output of the model)\n",
    "- $ w_1 $ is the **slope**.\n",
    "- $ w_0 $ is the **intercept**.\n",
    "\n",
    "### Multiple Linear Regression\n",
    "The equation stated can be generalized in **n-dimensional** feature space as shown in the following. The model is a **hyperplane** in this scenario.\n",
    "$$ \\hat{y} = w_nx_n + ... + w_1x_1 + w_0 $$\n",
    "\n",
    "Where $w = (w_0, w_1, ..., w_n)$ is the **weight (coefficient) vector**.\n",
    "\n",
    "<img src=\"../figures/figure_3_1_linear_regression.png\" alt=\"Linear Regression\">\n",
    "<p style=\"text-align:center; cursor:pointer;\"><a src=\"https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\">Source: Scikit-learn User Guide</a></p>\n",
    "\n",
    "### Ordinary Least Squares\n",
    "There are many different methods to solve the stated problem. **Ordinary Least Squares (OLS**) is the most common method used for fitting a linear regression model. Its goal is to find the line (or hyperplane) that **minimizes** the **total squared error** between the **predicted** values and the **actual** descrete data.\n",
    "\n",
    "A **loss function** (also called cost/error/objective function in the context of optimization) is a mathematical function that measures how well a machine learning model's predictions match the actual true values. There are different loss functions for different purposes that are explained in the next sections. **L2 Loss** (euclidean norm) is a fundamental loss function that measures the squared difference between predicted and true values. For a single data point it is expressed as follows.\n",
    "$$ \\text{Loss} = (t_\\text{true} - y_\\text{predicted}) ^ 2 $$\n",
    "\n",
    "For a dataset that contains a number of data points, **Mean Squared Error (MSE)** is typically used.\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n}{(y_i - \\hat{y}_i) ^ 2} $$\n",
    "\n",
    "### Problem Statement and Formulation\n",
    "In order to minimize the MSE function, the following optimization problem should be solved for the **decision variable vector** $w$.\n",
    "$$ w^* = \\underset{w \\in R^n}{\\arg\\min} \\, L(w) $$\n",
    "\n",
    "Where:\n",
    "- $ L $ is the **MSE function** that is supposed to be minimized.\n",
    "- $ w^* $ is the **minimizing vector**.\n",
    "\n",
    "While setting the derivative to zero finds **stationary points** in single-variable calculus, the **multivariate generalization** is shown in the following **gradient-based** equation.\n",
    "$$ ∇L = [\\frac{\\partial L}{\\partial w_0}, \\frac{\\partial L}{\\partial w_1}, ..., \\frac{\\partial L}{\\partial w_n}] ^ T = 0$$\n",
    "\n",
    "Where $∇$ is the **Laplace (gradient) operator (Laplacian)**.\n",
    "\n",
    "The stated set of equations is called **Normal Equations** in the context of machine learning that simultaneously sets all **partial derivatives** to zero, identifying flat regions in the parameter space where no single parameter adjustment can decrease the loss, which for convex functions like MSE guarantees finding the **global optimum** that defines the ordinary least squares solution.\n",
    "\n",
    "In simple linear regression, the normal equations can be solved for slope ($w_1$) and intercept ($w_0$) as shown in the following.\n",
    "$$ L = \\frac{1}{n} \\sum_{i=1}^{n}{(y_i - \\hat{y}_i) ^ 2} = \\frac{1}{n} \\sum_{i=1}^{n}{(y_i - w_1x + w_0) ^ 2} $$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial w_0} &= -2\\sum_{i=1}^{n} (y_i - w_1 x_i - w_0) = 0 \\\\\n",
    "\\frac{\\partial L}{\\partial w_1} &= -2\\sum_{i=1}^{n} x_i(y_i - w_1 x_i - w_0) = 0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Solving the coupled system yields the minimizing weights.\n",
    "$$ w_0 = \\bar{y} - w_1 \\bar{x} $$\n",
    "$$ w_1 = \\frac{\\sum (x_i - \\bar{x}) \\times \\sum (y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6d782-ff66-464b-bd4c-0400a7561491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d139a763-10ba-4743-a9d6-dfcd8073067d",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "**Optimization** forms the computational core of machine learning, framing model training as the search for parameters that **minimize a loss function**. Gradient descent is the foundational **iterative algorithm** that solves this problem. By calculating the gradient of the loss, it navigates the parameter space, taking steps proportional to the negative gradient to converge toward a **local minimum**. This simple yet powerful principle of following the path of **steepest descent** enables the learning process in models ranging from **simple linear regressions** to the most **complex deep neural networks**.\n",
    "\n",
    "### Problem Statement and Formulation\n",
    "In simple words, optimization means either **minimizing or maximizing** a function in the presence of **constriants**. The solution to the general minimization problem is given by the following equation.\n",
    "$$ x^* = \\underset{x \\in R^n}{\\arg\\min} \\, f(x) $$\n",
    "\n",
    "Where:\n",
    "- $ x = (x_0, x_1, …, x_n) $ is the **decision variable** vector.\n",
    "- $ f $ is the **objective/cost function** (also called loss function in machine learning problems) that is supposed to be minimized/maximized.\n",
    "- $ x^* $ is the **minimizing value**.\n",
    "\n",
    "Gradient descent is an iterative gradient-based optimization algorithm in which the key mathematical technique is the **first-order Taylor approximation** which is the **first two terms of the Taylor series expansion**. The full Taylor series for $f(x)$ around point $x_0$ is as follows.\n",
    "\n",
    "$$ f(x) = \\Sigma \\frac{f^{(n)}(x_0)}{n!}(x - x_0)^n$$\n",
    "\n",
    "The first two terms of this series are used for **linear approximation** of the function. This is called first-order Taylor approximation.\n",
    "\n",
    "$$ f(x) \\approx f(x_0) + f'(x_0)(x - x_0) $$\n",
    "\n",
    "In order to minimize $f(x)$, the starting point $x_0$ is selected. Since the linear approximation is valid **locally**, the small step $\\epsilon$ is taken from $x_0$. The goal is to find the best small step $\\epsilon$ that decreases $f(x)$ as much as possible. Thus, the function is **expanded linearly** around $x_0$ as shown in the following equation.\n",
    "\n",
    "$$ f(x_0 + \\epsilon) \\approx f(x_0) + f'(x_0)(x_0 + \\epsilon - x_0) $$\n",
    "$$ \\Rightarrow f(x_0 + \\epsilon) \\approx f(x_0) + \\epsilon f'(x_0) $$\n",
    "\n",
    "This can be extended to **multi-dimensional space** using the **Laplace (gradient) operator (Laplacian)**.\n",
    "\n",
    "$$ f(x + \\varepsilon) \\approx f(x) + \\nabla f(x) \\cdot \\varepsilon $$\n",
    "\n",
    "or in matrix notation \n",
    "\n",
    "$$ f(x + \\varepsilon) \\approx f(x) + \\nabla f(x)^T \\varepsilon $$\n",
    "\n",
    "The dot product $\\nabla f(x)^T \\varepsilon = ||\\nabla f(x)^T|| ||\\varepsilon|| \\cos(\\theta)$ is most negative when $\\varepsilon$ points in the **opposite direction** of the gradient. This means $\\theta = (2k + 1)\\pi$ and $\\cos(\\theta) = -1$. Thus, the **steepest descent** direction is as follows.\n",
    "\n",
    "$$ \\varepsilon = -\\alpha \\nabla f(x) $$\n",
    "\n",
    "Where $\\alpha > 0$ is called the **learning rate**.\n",
    "\n",
    "Finally, in the context of machine learning/deep learning, gradient descent can be expressed in a more common notation as shown in the following.\n",
    "$$ \\Delta w = -\\eta \\nabla L(w) $$\n",
    "$$ \\Rightarrow w^{t + 1} = w^t - \\eta \\nabla L(w^t) $$\n",
    "$$ w^0 = w^\\text{ initial} $$\n",
    "\n",
    "Where:\n",
    "- $ w $ is the **weight vector (paramethers)**.\n",
    "- $ \\eta $ is the **learning rate**.\n",
    "- $ L $ is the **loss function**.\n",
    "\n",
    "The following table provides a summary of common loss functions and their applications in machine learning.\n",
    "### Summary of Common Loss Functions\n",
    "| Loss Function | Formula | Type | Use Cases | Key Properties |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| **L1 Loss (MAE)** | `∑\\|y - ŷ\\|` | Regression | Robust regression, outliers present | Robust to outliers, non-differentiable at 0 |\n",
    "| **L2 Loss (MSE)** | `∑(y - ŷ)²` | Regression | Standard regression, smooth outputs | Sensitive to outliers, differentiable |\n",
    "| **RMSE** | `√(∑(y - ŷ)²/n)` | Regression | Regression (interpretable units) | Same units as target, sensitive to outliers |\n",
    "| **Binary Cross-Entropy** | `-[y·log(ŷ) + (1-y)·log(1-ŷ)]` | Classification | Binary classification | Probabilistic, penalizes wrong confidence |\n",
    "| **Categorical Cross-Entropy** | `-∑ y·log(ŷ)` | Classification | Multi-class classification | Multi-class generalization, softmax output |\n",
    "\n",
    "### Key Notes:\n",
    "- $\\hat{y}$ is the predicted output from the model\n",
    "- $y$ is the true/actual/desired (target) value (the ground truth from dataset)\n",
    "- $\\text{MSE}$ = Mean Squared Error, $\\text{MAE}$ = Mean Absolute Error\n",
    "- $L1$ is the absolute derrivation.\n",
    "- $L2$ is the euclidean norm.\n",
    "- $L1/L2$ can refer to either loss functions or regularization terms\n",
    "- **Cross-entropy** losses work with probability outputs (sigmoid/softmax)\n",
    "- Choice depends on problem type, outlier sensitivity, and optimization needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fb4e0-15db-4233-bbe7-b10259c737af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa34f11-a824-47d4-bb0f-5ad169a28adb",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] \"User Guide, Scikit-Learn Documentation\", https://scikit-learn.org/stable/user_guide.html, Accessed October 2025.\n",
    "\n",
    "[2] Goodfellow, Ian., Bengio, Yoshua., Courville, Aaron., \"Deep Learning\", MIT Press, 2016.\n",
    "\n",
    "[3] Grus, Joel., \"Data Science from Scratch\", O'Reilly, 2015.\n",
    "\n",
    "[4] Geiger, Andreas. \"Deep Learning\", Lecture, University of Tübingen, https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/deep-learning/, Accessed November 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6c336-6b99-4d70-a9ef-1291045584dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
